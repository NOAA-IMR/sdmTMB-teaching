---
title: "Delta models and index standardization"
format: html
editor: visual
execute: 
  echo: true
  eval: true
---

# Goals:

-   Practice fitting delta models
-   Understand how to inspect their more complicated model output
-   Learn how to plot aspects of component models and their combined predictions
-   Calculate an index and compare how model structure can impact an index 

```{r, message=FALSE, warning=FALSE}
#| echo=FALSE
library(sdmTMB)
library(dplyr)
library(ggplot2)
options(ggplot2.continuous.colour = "viridis")
options(ggplot2.continuous.fill = "viridis")
theme_set(theme_light())
```

# The data

We will continue working with the North Pacific Spiny Dogfish in the West Coast Vancouver Island synoptic trawl survey.

```{r}
dat <- readRDS(here::here("dfo-tesa-2023/data/wcvi-dogfish.rds"))
```

```{r}
head(dat)
```

Remember that the dataset contains sampling `depth` in meters and sample density `density` (CPUE) in units of tonnes/km^2^. Once again we will create new `log_depth` and `year_factor` variables to include as covariates in our models. We also need to add utms again.  

```{r}
dat$log_depth <- log(dat$depth)
dat$year_factor <- as.factor(dat$year)

dat <- add_utm_columns(dat, utm_crs = 3156, ll_names = c("longitude", "latitude"))
```

We will try working with a slightly coarser mesh today (`cutoff = 15`) because delta models fit more slowly. 

```{r}
mesh <- make_mesh(dat, xy_cols = c("X", "Y"), cutoff = 10)

ggplot() + inlabru::gg(mesh$mesh) + coord_fixed() +
  geom_point(aes(X, Y), data = dat, alpha = 0.2, size = 0.5)
```

### Exercise:

1.  Why do delta models fit more slowly? What is another reason why we might need a coarser mesh?

# Review fitting a tweedie model

We will start by refreshing our memory as to what the spatiotemporal models that we were fitting yesterday looked like. 
Remember that there was strong anisotropy? We also explored a couple different random field configurations but did not find a big difference between them. So, let's start here with a spatial random field and independent spatiotemporal (IID) random fields. 
Let's review what that model looked like. 

```{r, results='hide'}
fit_tweedie <- sdmTMB(
  density ~ 0 + year_factor + poly(log_depth, 2),
  data = dat,
  mesh = mesh,
  family = tweedie(link = "log"),
  spatial = "on",
  spatiotemporal = "iid",  
  time = "year", 
  anisotropy = TRUE
  # silent = FALSE
)
```

```{r}
sanity(fit_tweedie)
```

Did it have trouble? There are many reasons a model can suddenly have difficulty converging. What has changed this time? 
We only tried anisotropy with the AR1 model yesterday, and we may have used a different cutoff for our mesh. 
If there was a problem, try an extra optimization run and see if that's sufficient:

```{r}
fit_tweedie <- run_extra_optimization(fit_tweedie)
sanity(fit_tweedie)
```

```{r}
fit_tweedie
```

# Fitting a delta model

sdmTMB has the capability for built-in hurdle models (also called delta models). These are models with one model for zero vs. non-zero data and another component for the positive component. These models can also be implemented by fitting the two components separately and combining the predictions.

## Exercise: 

1. Why might one want to use a hurdle model instead of a Tweedie?

To fit a built-in hurdle model in sdmTMB, you simply need to specify a `family` that starts with `delta_`. For this exercise with will focus on the `delta_gamma`. This fits a binomial presence-absence model (i.e., binomial(link = "logit")) and then a model for the positive catches only with a Gamma observation distribution and a log link (i.e., Gamma(link = "log")). Here and with other delta models, the link1 and link2 can be omitted and left at their default values.

We will run this without anisotropy for now, because this more complex model has difficulty estimating this more flexible spatial field.

```{r, results='hide'}
fit_dg <- sdmTMB(
  density ~ 0 + year_factor + poly(log_depth, 2),
  data = dat,
  mesh = mesh,
  family = delta_gamma(link1 = "logit", link2 = "log"), #< new
  spatial = "on",
  spatiotemporal = "iid",  
  time = "year", 
  anisotropy = FALSE #< new
  # silent = FALSE
)
```


```{r}
sanity(fit_dg)
fit_dg
```

## Exercise:

1. How does this model output differ from that of the tweedie?
2. What differences do you notice between parameter estimates in the two component models of the delta model?
3. Are any parameters missing from one model or the other? If so, why?
4. Why did we choose a delta_gamma family rather than one of the other options?


### Take a closer look at parameter estimates

```{r}
tidy(fit_dg, conf.int = TRUE, model = 1)
tidy(fit_dg, conf.int = TRUE, model = 2)
```
```{r}
tidy(fit_dg, "ran_pars", conf.int = TRUE, model = 1)
tidy(fit_dg, "ran_pars", conf.int = TRUE, model = 2)
```

## Exercise:

1. Which estimates have estimated well in each component model and which have not?
2. Can you think of reasons why some values are difficult to estimate?
2. What could we do to improve them?


# Delta models with different model structures between the component models

The default in built-in delta models for the formula, spatial and spatio-temporal structure, and anisotropy to be shared between the two model components. However, some elements (formula, spatial, spatiotemporal, and share_range) can also be specified independently for each model using a `list()` format within the function argument (see examples below). The first element of the list is for the binomial component and the second element is for the positive component (e.g., Gamma). Some formula elements must be shared for now (e.g., smoothers, random intercepts, spatially varying coefficients, and time-varying coefficients).

Anisotropy cannot be separately specified for each component model; so when anisotropy is included, it is by default shared across the two model components. However it can be made unique in each model component by using `control = sdmTMBcontrol(map = list(ln_H_input = factor(c(1, 2, 3, 4))))` and adding the argument control when fitting the model. This ‘maps’ the anisotropy parameters be unique across model components. Similarly, `map = list(ln_H_input = factor(c(NA, NA, 1, 2)))` will allow only model 2 to include anisotropy. The default is `map = list(ln_H_input = factor(c(1, 2, 1, 2)))`.

Given that we know anisotropy was important for the Tweedie models from yesterday, maybe we can find a configuration of the delta model that can estimate it. One option is to turn off the spatiotemporal fields in the binomial component. Turns out that is sufficient to allow this model to converge, and that adding any additional flexibility caused the model to have difficulty estimating the anisotropy parameters.   

```{r}
fit_dg_aniso <- update(fit_dg,
                       spatiotemporal = list("off", "iid"), #< new
                       anisotropy = TRUE
                       )
```

```{r}
sanity(fit_dg_aniso)
fit_dg_aniso
```
```{r}
plot_anisotropy(fit_tweedie)
plot_anisotropy(fit_dg_aniso)
```

```{r}
AIC(fit_tweedie, fit_dg)

fit_dg_reml <- update(fit_dg, reml = TRUE)
fit_aniso_reml <- update(fit_dg_aniso, reml = TRUE)

AIC(fit_dg_reml, fit_aniso_reml)
```


# Visualizing delta models

For easy exploration of different models, we can rename any of our models `fit`, but in this case I will choose the one with the lowest AIC.

```{r}
fit <- fit_dg
```


### Checking residuals 

We can calculate randomized quantile residuals with the `residuals.sdmTMB()` function and plot them against possible confounding covariates, whether they were included in the model or not:

```{r}
dat$resid <- residuals(fit)

ggplot(dat, aes(log_depth, resid)) +
  geom_point(size = 0.5) 
```

We can also plot those residuals spatially:

```{r}
ggplot(dat, aes(X, Y, colour = resid)) +
  facet_wrap(~year) +
  geom_point(size = 0.5) +
  coord_fixed() +
  scale_colour_gradient2()
```


### Covariate effects

Now, let's inspect the covariate effects of our delta model "m". The easiest way to check the relationships within each model is to use the sdmTMB function `visreg_delta()` to pass each component model separately to visreg. The argument `type = "conditional"` (the default) holds other variables constant at the median for numeric variables and the most common category for factors. 
The `ggeffects::ggeffect()` function doesn't work with delta models yet.


```{r}
visreg_delta(fit, xvar = "year_factor", model = 1, type = "conditional")
```

```{r}
visreg_delta(fit, xvar = "year_factor", model = 2)
```

## Exercise: 

1. Which link space are each of the above plots in? Can you remember how to put them in natural or response space?
2. Can you produce similar plots in response space for the depth effect from both component models?

```{r, echo=FALSE}

```


Now, if you have the same covariate in both models, you might wish to plot the effect of that covariate on your overall predictions.
Like before, we can create a data frame for our covariate of interest and use the predict function. 
Currently, we cannot use `se_fit = TRUE` to get combined uncertainty for a delta model, so the resulting data frame contains predictions in link space for each component model:

```{r}
nd <- data.frame(log_depth = seq(log(50), log(700), length.out = 100), year = 2004)
nd$year_factor <- as.factor(nd$year)

p <- predict(
  fit,
  newdata = nd,
  model = NA,
  re_form = ~ 0 
)

head(p)
```

To combine them we must transform them into response space and then multiply them together:

```{r}
p$bin_prob <- fit$family[[1]]$linkinv(p$est1)
p$pos_exp <- fit$family[[2]]$linkinv(p$est2)
p$est_exp <- p$bin_prob * p$pos_exp

head(p)
```

```{r}
ggplot(p, aes(log_depth, est_exp)) +
  geom_line()
```

But if a measure of uncertainty is required, we have to simulate from the joint parameter precision matrix using the predict() function with any number of simulations selected (e.g., sims = 500). Because the predictions come from simulated draws from the parameter covariance matrix, the predictions will become more consistent with a larger number of draws. However, a greater number of draws takes longer to calculate and will use more memory (larger matrix), so fewer draws (~100) may be fine for experimentation. A larger number (say ~1000) may be appropriate for final model runs.


```{r}
set.seed(28239)
p_bin_sim <- predict(fit, newdata = nd, re_form = ~ 0, model = 1, nsim = 100)
p_pos_sim <- predict(fit, newdata = nd, re_form = ~ 0, model = 2, nsim = 100)

p_bin_prob_sim <- fit$family[[1]]$linkinv(p_bin_sim)
p_pos_exp_sim <- fit$family[[2]]$linkinv(p_pos_sim)
p_combined_sim <- p_bin_prob_sim * p_pos_exp_sim

nd$pred <- apply(p_combined_sim, 1, median)
nd$lwr <- apply(p_combined_sim, 1, function(x) quantile(x, 0.025))
nd$upr <- apply(p_combined_sim, 1, function(x) quantile(x, 0.975))

ggplot(nd, aes(log_depth, pred,
  ymin = lwr,
  ymax = upr)) +
  geom_ribbon(alpha = 0.4) +
  geom_line()
```

Exercise:

1. Can you remember what link each component of this model used?
2. If you have forgotten, or in case you do sometime, can you figure out what they are from the model object?


### Mapping delta model predictions

First, we need to again generate an appropriate prediction grid with all the same time elements as our model data:

```{r}
wcvi_grid <- readRDS(here::here("dfo-tesa-2023/data/wcvi-grid.rds"))
head(wcvi_grid)
wcvi_grid$log_depth <- log(wcvi_grid$depth)
grid <- purrr::map_dfr(unique(dat$year), ~ tibble(wcvi_grid, year = .x))
grid$year_factor <- as.factor(grid$year)
```

If we add type = "response" to our usual function call, we get all the usual components in link space, as well as a combined estimate in natural space. 

```{r}
p <- predict(fit, newdata = grid, type = "response")

head(p)
```

Exercise:

1. Which of these columns do you think represents the combined prediction?
2. How does this data frame of predictions differ from what we would have gotten when predicting from a Tweedie model?
3. Which of these columns are in link space? If in link space, do you know what kind?
4. Would a data frame of predictions from your "fit_dg_custom" model have the same number of columns? Predict from the other model to check if you were right.


We can plot each of the components spatially:

```{r}
plot_map <- function(data, column_name) {
  ggplot(data, aes(X, Y, fill = {{ column_name }})) +
    facet_wrap(~year) +
    geom_raster() +
    coord_fixed()
}
```


```{r}
plot_map(p, est1) +
    scale_fill_viridis_c()

plot_map(p, est2) +
    scale_fill_viridis_c(trans = "log")
```

```{r}
# Depth and year effect contribution transformed into natural space:
# (Everything not a random field)
plot_map(p, plogis(est_non_rf1)) +
    scale_fill_viridis_c()

plot_map(p, exp(est_non_rf2)) +
    scale_fill_viridis_c()
```


```{r}
# Spatial random field:
plot_map(p, omega_s1) +
  scale_fill_gradient2() 

plot_map(p, omega_s2) +
  scale_fill_gradient2() 
```

```{r}
# Spatial-temporal random field:
plot_map(p, epsilon_st1) +
  scale_fill_gradient2() 

plot_map(p, epsilon_st2) +
  scale_fill_gradient2() 
```


```{r}
plot_map(p, est) +
    scale_fill_viridis_c(trans = "log")
```


# Index standardization

To calculate an index from any of these models we need to run our `predict.sdmTBM()` function with the argument `return_tmb_object = TRUE`. We can then run our get_index() function to extract the total biomass calculations and standard errors.

We can set the area argument to our cell_area column in km2. In this case the value is 4 km2 for all of the cells, since our grid cells are 2 km x 2 km. If some grid cells were not fully in the survey domain (or were on land), we could feed a vector of grid areas to the area argument that matched the number of grid cells. Because the density units are tonnes per km2 for this data, the index is in tonnes. 


```{r}
p_dg <- predict(fit, newdata = grid, return_tmb_object = TRUE)
index_dg <- get_index(p_dg, area = grid$cell_area, bias_correct = FALSE)

ggplot(index_dg, aes(year, est)) + geom_line() +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.4) +
  xlab('Year') + ylab('Biomass estimate (tonnes)')
```

Let's see if this index differs from what it would look like using the Tweedie model. 

```{r}
p_tw <- predict(fit_tweedie, newdata = grid, return_tmb_object = TRUE)
index_tw <- get_index(p_tw, area = grid$cell_area, bias_correct = FALSE)

index_dg$Method <- "Delta-gamma"
index_tw$Method <- "Tweedie"

bind_rows(index_dg, index_tw) %>% 
  ggplot(aes(year, est, fill = Method)) + geom_line(aes(colour = Method)) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.4) +
  xlab('Year') + ylab('Biomass estimate (tonnes)')
```

We used `bias_correction = FALSE` to speed things up, but for any final result you will need it. Let's see how much it changes with the simpler (and therefore faster) Tweedie model. 

```{r}
index_twc <- get_index(p_tw, area = grid$cell_area, bias_correct = TRUE)

index_twc$Method <- "Bias correction"

bind_rows(index_tw, index_twc) %>% 
  ggplot(aes(year, est, fill = Method)) + geom_line(aes(colour = Method)) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.4) +
  xlab('Year') + ylab('Biomass estimate (tonnes)')
```

# Bonus: 

1. If a measure of uncertainty for the spatial predictions was required, we could use simulations draws the same way we did for our combined depth prediction to calculate a coefficient of variation (sd(x)/mean(x)).  See if you can modify the code from before to get a map of the CV.  

```{r, echo=FALSE}
## the code for the depth prediction
# p_bin_sim <- predict(fit, newdata = nd, re_form = ~ 0, model = 1, nsim = 100)
# p_pos_sim <- predict(fit, newdata = nd, re_form = ~ 0, model = 2, nsim = 100)
# 
# p_bin_prob_sim <- fit$family[[1]]$linkinv(p_bin_sim)
# p_pos_exp_sim <- fit$family[[2]]$linkinv(p_pos_sim)
# p_combined_sim <- p_bin_prob_sim * p_pos_exp_sim
# 
# nd$pred <- apply(p_combined_sim, 1, median)
# nd$lwr <- apply(p_combined_sim, 1, function(x) quantile(x, 0.025))
# nd$upr <- apply(p_combined_sim, 1, function(x) quantile(x, 0.975))
# 
# ggplot(nd, aes(log_depth, pred,
#   ymin = lwr,
#   ymax = upr)) +
#   geom_ribbon(alpha = 0.4) +
#   geom_line()
```

