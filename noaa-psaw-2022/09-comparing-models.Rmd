---
title: "Model selection with sdmTMB"
subtitle: "NOAA PSAW Seminar Series"
author: ""
institute: ""
date: "March 9, 2022"
output:
  xaringan::moon_reader:
    css: ["xaringan-themer.css", "theme.css"]
    lib_dir: libs
    nature:
      highlightLines: true
      countIncrementalSlides: false
---

<!-- Build with: xaringan::inf_mr() -->

```{r preamble, include=FALSE, cache=FALSE}
source(here::here("noaa-psaw-2022/preamble.R"))
do.call(knitr::opts_chunk$set, knitr_opts)
```

```{r libs, include=FALSE}
library(dplyr)
library(sdmTMB)
library(ggplot2)
```

# What is the goal of modeling? 

* Straight prediction?
  * in sample, out of sample, both?

* Parsimony?
  * balance of bias-variance tradeoff

* Not all metrics appropriate for all applications

---

# Too many metrics to discuss

* Root mean squared error 

* AUC (see pseudo-absence example)
  * true and false positive rates

* AIC: widely used tool in ecology
  * $\mathrm{AIC} = -2 \log L + 2K$
  * designed for fixed effects models (Burnham and Anderson 2002)
  
---

# AIC and likelihood with sdmTMB

* `AIC()` and `logLik()` functions work like `glm`  

```{r echo=TRUE, eval=FALSE}
mesh <- make_mesh(pcod,
  xy_cols = c("X", "Y"),
  cutoff = 10
)
fit <- sdmTMB(
  present ~ 1,
  data = pcod,
  mesh = mesh,
  family = binomial(link = "logit"),
  spatial = "on"
)
logLik(fit) # log likelihood
AIC(fit) # AIC
```

---

# When to use restricted maximum likelihood (REML) 

* Don't use REML when comparing alternative fixed effects with same random effects structure (Zuur et al. 2009)  

* Use REML when comparing different random effect structures

* Where is REML in sdmTMB?

```{r echo=TRUE, eval=FALSE}
fit <- sdmTMB(
  ...,
  reml = FALSE,
  ...
)
```

---

# Reminder: fixed and random effects in sdmTMB

Random effects include

* random intercepts, `(1 | group)`
* smooth terms, `s(year)`
* time-varying coefficients
* spatially varying coefficients
* all random fields

---

# Limitations of AIC

* Originally conceived for fixed effects 
  * Burnham and Anderson (2002)

* Approximate & problematic for mixed effects
  * Vaida and Blanchard (2005)
  * Liang et al. (2008)
  
* Great FAQ on `glmmTMB` by Ben Bolker
  *[link here](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#can-i-use-aic-for-mixed-models-how-do-i-count-the-number-of-degrees-of-freedom-for-a-random-effect)


---

# Predictive model selection

* Ideal world: use cross validation to evaluate predictive accuracy

* Split data into train / test sets

* Objective function:
  * maximize log-likelihood of test data

---

# ELPD

* expected log pointwise predictive density
  * interpreted as theoretical expectation for new dataset  
  
ELPD: $\frac{\log ( \sum( \exp( \log L(y^*|\theta))))} {N}$

Vehtari, Gelman, and Gabry (2017)  

---

# Cross validation in sdmTMB

```{r echo=FALSE}
mesh <- make_mesh(pcod,
  xy_cols = c("X", "Y"),
  cutoff = 10
)
```

.small[
```{r echo=TRUE}
fit <- sdmTMB_cv( #<<
  present ~ 1,
  data = pcod,
  mesh = mesh,
  family = binomial(link = "logit"),
  k_folds = 8, #<<
  fold_ids = NULL, #<<
  parallel = TRUE, #<<
  use_initial_fit = FALSE #<<
)
```
]

* More folds = more computation time
* `use_initial_fit = TRUE` 
  * fits first fold, and initializes subsequent model fits from that  

---

# Cross validation in sdmTMB

`sdmTMB_cv()` returns:  

A list of models (each `sdmTMB` object)  

`fold_loglik`: sum of held out likelihoods for each fold

`sum_loglik`: sum across `fold_loglik`, or all data

`fold_elpd`: expected log predictive density for each fold

`elpd`: Expected log predictive density across all data

---

# How to choose folds? How many?

.small[
* Words of wisdom:
  * Can be gighly variable based on data, folds, sampling scheme
  
* Spatial sampling or random?
  * `blockCV` R package, Valavi et al. (2019)
  
* How to sample with time / years? 
  * LOOCV (leave-one-out...) v. LFOCV (leave-future-out...)

* `sdmTMB_cv()` does random fold assignment
  * Custom folds specified with `fold_ids`
]

---

# Illustration

Test data creation agnostic to space

```{r echo = TRUE, eval=TRUE}
set.seed(123)
data(pcod)
test <- pcod %>%
  group_by(year) %>%
  sample_n(25) # 230 - 250 samples / year
```

---

# Illustration

```{r fig.asp=0.6}
ggplot(test, aes(X, Y)) +
  geom_point() +
  facet_wrap(~year)
```


---

# Ensembling

Rather than choose best model, we can do model averaging

* Ensemble may perform better than any single model  

* AIC weights (please don't!)

* Stacking

---

# Stacking

* Borrowed from Bayesian literature, e.g. Yao et al. (2018)

* Given set of models, $m_{1}, m_{2}, ..., m_{M}$

* Estimate weights to maximize combined predictive density

$$p(y^* | y) = \sum_{i=1}^{M} w_{i} p(y^*|y, m_{i})$$

---

# Stacking in sdmTMB

```{r eval = FALSE, echo=TRUE}
sdmTMB_stacking(model_list,
  include_folds = NULL
)
```

* `model_list` is a collection of models fit with `sdmTMB_cv`

* `optim()` is used to find the best weights $w_{i}$ to maximize the combined predictive density

* Not in `main` branch yet

---

# Using future

sdmTMB has built-in functionality for cross-validation. If we were to set a `future::plan()`, the folds would be fit in parallel:

```{r cv, warning=FALSE}
mesh <- make_mesh(pcod, c("X", "Y"), cutoff = 10)
## Set parallel processing if desired:
# library(future)
# plan(multisession)
m_cv <- sdmTMB_cv(
  density ~ s(depth, k = 5),
  data = pcod, mesh = mesh,
  family = tweedie(link = "log"), k_folds = 2
)
# Sum of log likelihoods of left-out data:
m_cv$sum_loglik
# Expected log pointwise predictive density from left-out data:
# (average likelihood density)
m_cv$elpd
```

See [`?sdmTMB_cv`](https://pbs-assess.github.io/sdmTMB/reference/sdmTMB_cv.html) for more details.

---
